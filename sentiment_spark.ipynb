{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PRODUCT CATEGORY CLASSIFICATION**\n",
    "\n",
    "### Here, we want to develop an automatic and scalable first prototipe that helps to correctly categorize a new product in the available categories when it arrives.\n",
    "\n",
    "This first prototipe will help us to identify in what elements we have to go deeper in order to get the best model.\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/natural-language-processing-in-apache-spark-using-nltk-part-2-2-5550b85f3340"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary of requirements**\n",
    "\n",
    "1. Train a model that predicts the product category for Software, Digital Software, and\n",
    "Digital Video Games products using the Amazon Customer Reviews dataset.\n",
    "2. Evaluate and validate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I checked warnings, but for the final report I prefer ignore those \n",
    "#that really does not affect the results (warnings of libraries, etc)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load all the required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /home/erikapat/anaconda3/lib/python3.7/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/erikapat/anaconda3/lib/python3.7/site-packages (from wordcloud) (1.17.1)\n",
      "Requirement already satisfied: pillow in /home/erikapat/anaconda3/lib/python3.7/site-packages (from wordcloud) (6.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud \n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Create spark session and provide master as yarn-client and provide application name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration properties of Apache Spark\n",
    "#sc.stop()\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "APP_NAME = 'pyspark_python'\n",
    "MASTER = 'local[*]'\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "conf = conf.setMaster(MASTER)\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Load amazon_alexa.tsv(TSV stands for Tab Separated Values) data into spark dataframe from HDFS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema = StructType([\n",
    "    StructField(\"marketplace\",  StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\",  StringType(), True),\n",
    "    StructField(\"product_parent\", IntegerType(), True),\n",
    "    StructField(\"product_title\", StringType(), True),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"star_rating\", IntegerType(), True),\n",
    "    StructField(\"helpful_votes\", IntegerType(), True),\n",
    "    StructField(\"total_votes\", IntegerType(), True),\n",
    "    StructField(\"vine\", StringType(), True),\n",
    "    StructField(\"verified_purchase\", StringType(), True),\n",
    "    StructField(\"review_headline\", StringType(), True),\n",
    "    StructField(\"review_body\", StringType(), True),\n",
    "    StructField(\"review_date\", StringType(), True)])\n",
    "'''\n",
    "df_video_games = spark.read\\\n",
    "    .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "    .option(\"delimiter\",\",\")\\\n",
    "    .option(\"inferSchema\", \"True\")\\\n",
    "    .option(\"header\", \"True\")\\\n",
    "    .load('data/amazon_reviews_us_Digital_Video_Games_v1_00.tsv')\n",
    "'''\n",
    "\n",
    "df_video_games = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option(\"delimiter\",\"\\t\")\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load('data/amazon_reviews_us_Digital_Video_Games_v1_00.tsv')\n",
    "\n",
    "df_software = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option(\"delimiter\",\"\\t\")\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load('data/amazon_reviews_us_Software_v1_00.tsv')\n",
    "\n",
    "df_digital_software = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option(\"delimiter\",\"\\t\")\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load('data/amazon_reviews_us_Digital_Software_v1_00.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MERGE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_video_games.union(df_software);\n",
    "df = df.union(df_digital_software);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|        review_date|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
      "|         US|   21269168| RSH1OZ87OYK92|B013PURRZW|     603406193|Madden NFL 16 - X...|Digital_Video_Games|          2|            2|          3|   N|                N|A slight improvem...|I keep buying mad...|2015-08-31 00:00:00|\n",
      "|         US|     133437|R1WFOQ3N9BO65I|B00F4CEHNK|     341969535| Xbox Live Gift Card|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Awesome|2015-08-31 00:00:00|\n",
      "|         US|   45765011| R3YOOS71KM5M9|B00DNHLFQA|     951665344|Command & Conquer...|Digital_Video_Games|          5|            0|          0|   N|                Y|Hail to the great...|If you are preppi...|2015-08-31 00:00:00|\n",
      "|         US|     113118|R3R14UATT3OUFU|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Perfect|2015-08-31 00:00:00|\n",
      "|         US|   22151364| RV2W9SGDNQA2C|B00G9BNLQE|     640460561|Saints Row IV - E...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!|2015-08-31 00:00:00|\n",
      "|         US|   22151364|R3CFKLIZ0I2KOB|B00IMIL498|     621922192|Double Dragon: Ne...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!|2015-08-31 00:00:00|\n",
      "|         US|   38426028|R1LRYU1V0T3O38|B00S00IJH8|     215163395|              Sims 4|Digital_Video_Games|          4|            0|          0|   N|                Y|i like the new sk...|i like the new sk...|2015-08-31 00:00:00|\n",
      "|         US|    6057518| R44QKV6FE5CJ2|B004RMK4BC|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Super|2015-08-31 00:00:00|\n",
      "|         US|   20715661|R2TX1KLPXXXNYS|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|         Easy & Fast|Excellent, fast a...|2015-08-31 00:00:00|\n",
      "|         US|   26540306|R1JEEW4C6R89BA|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|                  Ok|2015-08-31 00:00:00|\n",
      "|         US|    8926809|R3B3UHK1FO0ERS|B004774IPU|     151985175|Sid Meier's Civil...|Digital_Video_Games|          1|            0|          0|   N|                N|I am still playin...|As has been writt...|2015-08-31 00:00:00|\n",
      "|         US|   31525534|R2GVSDHW513SS1|B002LIT9EC|     695277014|Build-a-lot 4: Po...|Digital_Video_Games|          5|            0|          0|   N|                Y|Probably the best...|Probably the best...|2015-08-31 00:00:00|\n",
      "|         US|   22151364|R1R1NT516PYT73|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!|2015-08-31 00:00:00|\n",
      "|         US|   22977584|R3K624QDQKENN9|B010KYDNDG|     835376637|Minecraft for PC/...|Digital_Video_Games|          4|            0|          0|   N|                Y|                 FUN|COOL BUT IT LAGES...|2015-08-31 00:00:00|\n",
      "|         US|   45765011| R1FOXH7PCJX3V|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          1|            0|          2|   N|                Y|            One Star|Lames purchase I ...|2015-08-31 00:00:00|\n",
      "|         US|    2239522| RA1246M1OMDWC|B004RMK4P8|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Great|2015-08-31 00:00:00|\n",
      "|         US|   48805811|R2I9SXWB0PAEKQ|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|          Awesome!!!|2015-08-31 00:00:00|\n",
      "|         US|   18646481|R3UGL544NA0G9C|B00BI16Z22|     552981447|Brink of Consciou...|Digital_Video_Games|          4|            0|          0|   N|                Y|       worth playing|pretty good but n...|2015-08-31 00:00:00|\n",
      "|         US|   10310935|R1CBA4Y92GVAVM|B004VSTQ2A|     232803743|Xbox Live Subscri...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|What can I say......|2015-08-31 00:00:00|\n",
      "|         US|    5587610|R24NEKNR01VEHU|B00GAC1D2G|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|        Just amazing|Very fast to rece...|2015-08-31 00:00:00|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>21269168</td>\n",
       "      <td>RSH1OZ87OYK92</td>\n",
       "      <td>B013PURRZW</td>\n",
       "      <td>603406193</td>\n",
       "      <td>Madden NFL 16 - Xbox One Digital Code</td>\n",
       "      <td>Digital_Video_Games</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>A slight improvement from last year.</td>\n",
       "      <td>I keep buying madden every year hoping they ge...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>133437</td>\n",
       "      <td>R1WFOQ3N9BO65I</td>\n",
       "      <td>B00F4CEHNK</td>\n",
       "      <td>341969535</td>\n",
       "      <td>Xbox Live Gift Card</td>\n",
       "      <td>Digital_Video_Games</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Awesome</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>45765011</td>\n",
       "      <td>R3YOOS71KM5M9</td>\n",
       "      <td>B00DNHLFQA</td>\n",
       "      <td>951665344</td>\n",
       "      <td>Command &amp; Conquer The Ultimate Collection [Ins...</td>\n",
       "      <td>Digital_Video_Games</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Hail to the great Yuri!</td>\n",
       "      <td>If you are prepping for the end of the world t...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>113118</td>\n",
       "      <td>R3R14UATT3OUFU</td>\n",
       "      <td>B004RMK5QG</td>\n",
       "      <td>395682204</td>\n",
       "      <td>Playstation Plus Subscription</td>\n",
       "      <td>Digital_Video_Games</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>22151364</td>\n",
       "      <td>RV2W9SGDNQA2C</td>\n",
       "      <td>B00G9BNLQE</td>\n",
       "      <td>640460561</td>\n",
       "      <td>Saints Row IV - Enter The Dominatrix [Online G...</td>\n",
       "      <td>Digital_Video_Games</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Awesome!</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     21269168   RSH1OZ87OYK92  B013PURRZW       603406193   \n",
       "1          US       133437  R1WFOQ3N9BO65I  B00F4CEHNK       341969535   \n",
       "2          US     45765011   R3YOOS71KM5M9  B00DNHLFQA       951665344   \n",
       "3          US       113118  R3R14UATT3OUFU  B004RMK5QG       395682204   \n",
       "4          US     22151364   RV2W9SGDNQA2C  B00G9BNLQE       640460561   \n",
       "\n",
       "                                       product_title     product_category  \\\n",
       "0              Madden NFL 16 - Xbox One Digital Code  Digital_Video_Games   \n",
       "1                                Xbox Live Gift Card  Digital_Video_Games   \n",
       "2  Command & Conquer The Ultimate Collection [Ins...  Digital_Video_Games   \n",
       "3                      Playstation Plus Subscription  Digital_Video_Games   \n",
       "4  Saints Row IV - Enter The Dominatrix [Online G...  Digital_Video_Games   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            2              2            3    N                 N   \n",
       "1            5              0            0    N                 Y   \n",
       "2            5              0            0    N                 Y   \n",
       "3            5              0            0    N                 Y   \n",
       "4            5              0            0    N                 Y   \n",
       "\n",
       "                        review_headline  \\\n",
       "0  A slight improvement from last year.   \n",
       "1                            Five Stars   \n",
       "2               Hail to the great Yuri!   \n",
       "3                            Five Stars   \n",
       "4                            Five Stars   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0  I keep buying madden every year hoping they ge...  2015-08-31  \n",
       "1                                            Awesome  2015-08-31  \n",
       "2  If you are prepping for the end of the world t...  2015-08-31  \n",
       "3                                            Perfect  2015-08-31  \n",
       "4                                           Awesome!  2015-08-31  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['marketplace', 'customer_id', 'review_id', 'product_id',\n",
    "       'product_parent', 'product_title', 'product_category', 'star_rating',\n",
    "       'helpful_votes', 'total_votes', 'vine', 'verified_purchase',\n",
    "       'review_headline', 'review_body', 'review_date']\n",
    "df = df.toDF(*names)\n",
    "df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Fetch column: “verified_reviews” because we need only that column for extracting sentiments from customers and for that we need to convert our dataframe into RDD(best suited for processing unstructured data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_rdd = df.select(\"review_body\").rdd.flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 4: Remove header and convert all the data into lowercase for easy processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I keep buying madden every year hoping they get back to football. This years version is a little better than last years -- but that's not saying much.The game looks great. The only thing wrong with the animation, is the way the players are always tripping on each other.<br /><br />The gameplay is still slowed down by the bloated pre-play controls. What used to take two buttons is now a giant PITA to get done before an opponent snaps the ball or the play clock runs out.<br /><br />The turbo button is back, but the player movement is still slow and awkward. If you liked last years version, I'm guessing you'll like this too. I haven't had a chance to play anything other than training and a few online games, so I'm crossing my fingers and hoping the rest is better.<br /><br />The one thing I can recommend is NOT TO BUY THE MADDEN BUNDLE. The game comes as a download. So if you hate it, there's no trading it in at Gamestop.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = reviews_rdd.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_rmv_col = reviews_rdd.filter(lambda row: row != header)\n",
    "\n",
    "lowerCase_sentRDD = reviews_rdd.map(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i keep buying madden every year hoping they get back to football. this years version is a little better than last years -- but that's not saying much.the game looks great. the only thing wrong with the animation, is the way the players are always tripping on each other.<br /><br />the gameplay is still slowed down by the bloated pre-play controls. what used to take two buttons is now a giant pita to get done before an opponent snaps the ball or the play clock runs out.<br /><br />the turbo button is back, but the player movement is still slow and awkward. if you liked last years version, i'm guessing you'll like this too. i haven't had a chance to play anything other than training and a few online games, so i'm crossing my fingers and hoping the rest is better.<br /><br />the one thing i can recommend is not to buy the madden bundle. the game comes as a download. so if you hate it, there's no trading it in at gamestop.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowerCase_sentRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 5: Text data can be split into sentences and this process is called sentence tokenization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_TokenizeFunct(x):\n",
    "    return nltk.sent_tokenize(x)\n",
    "sentenceTokenizeRDD = lowerCase_sentRDD.map(sent_TokenizeFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i keep buying madden every year hoping they get back to football. this years version is a little better than last years -- but that's not saying much.the game looks great. the only thing wrong with the animation, is the way the players are always tripping on each other.<br /><br />the gameplay is still slowed down by the bloated pre-play controls. what used to take two buttons is now a giant pita to get done before an opponent snaps the ball or the play clock runs out.<br /><br />the turbo button is back, but the player movement is still slow and awkward. if you liked last years version, i'm guessing you'll like this too. i haven't had a chance to play anything other than training and a few online games, so i'm crossing my fingers and hoping the rest is better.<br /><br />the one thing i can recommend is not to buy the madden bundle. the game comes as a download. so if you hate it, there's no trading it in at gamestop.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowerCase_sentRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 6: Now split each sentence into words, also called word tokenization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_TokenizeFunct(x):\n",
    "    splitted = [word for line in x for word in line.split()]\n",
    "    return splitted\n",
    "wordTokenizeRDD = sentenceTokenizeRDD.map(word_TokenizeFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'keep',\n",
       " 'buying',\n",
       " 'madden',\n",
       " 'every',\n",
       " 'year',\n",
       " 'hoping',\n",
       " 'they',\n",
       " 'get',\n",
       " 'back',\n",
       " 'to',\n",
       " 'football.',\n",
       " 'this',\n",
       " 'years',\n",
       " 'version',\n",
       " 'is',\n",
       " 'a',\n",
       " 'little',\n",
       " 'better',\n",
       " 'than',\n",
       " 'last',\n",
       " 'years',\n",
       " '--',\n",
       " 'but',\n",
       " \"that's\",\n",
       " 'not',\n",
       " 'saying',\n",
       " 'much.the',\n",
       " 'game',\n",
       " 'looks',\n",
       " 'great.',\n",
       " 'the',\n",
       " 'only',\n",
       " 'thing',\n",
       " 'wrong',\n",
       " 'with',\n",
       " 'the',\n",
       " 'animation,',\n",
       " 'is',\n",
       " 'the',\n",
       " 'way',\n",
       " 'the',\n",
       " 'players',\n",
       " 'are',\n",
       " 'always',\n",
       " 'tripping',\n",
       " 'on',\n",
       " 'each',\n",
       " 'other.<br',\n",
       " '/><br',\n",
       " '/>the',\n",
       " 'gameplay',\n",
       " 'is',\n",
       " 'still',\n",
       " 'slowed',\n",
       " 'down',\n",
       " 'by',\n",
       " 'the',\n",
       " 'bloated',\n",
       " 'pre-play',\n",
       " 'controls.',\n",
       " 'what',\n",
       " 'used',\n",
       " 'to',\n",
       " 'take',\n",
       " 'two',\n",
       " 'buttons',\n",
       " 'is',\n",
       " 'now',\n",
       " 'a',\n",
       " 'giant',\n",
       " 'pita',\n",
       " 'to',\n",
       " 'get',\n",
       " 'done',\n",
       " 'before',\n",
       " 'an',\n",
       " 'opponent',\n",
       " 'snaps',\n",
       " 'the',\n",
       " 'ball',\n",
       " 'or',\n",
       " 'the',\n",
       " 'play',\n",
       " 'clock',\n",
       " 'runs',\n",
       " 'out.<br',\n",
       " '/><br',\n",
       " '/>the',\n",
       " 'turbo',\n",
       " 'button',\n",
       " 'is',\n",
       " 'back,',\n",
       " 'but',\n",
       " 'the',\n",
       " 'player',\n",
       " 'movement',\n",
       " 'is',\n",
       " 'still',\n",
       " 'slow',\n",
       " 'and',\n",
       " 'awkward.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'liked',\n",
       " 'last',\n",
       " 'years',\n",
       " 'version,',\n",
       " \"i'm\",\n",
       " 'guessing',\n",
       " \"you'll\",\n",
       " 'like',\n",
       " 'this',\n",
       " 'too.',\n",
       " 'i',\n",
       " \"haven't\",\n",
       " 'had',\n",
       " 'a',\n",
       " 'chance',\n",
       " 'to',\n",
       " 'play',\n",
       " 'anything',\n",
       " 'other',\n",
       " 'than',\n",
       " 'training',\n",
       " 'and',\n",
       " 'a',\n",
       " 'few',\n",
       " 'online',\n",
       " 'games,',\n",
       " 'so',\n",
       " \"i'm\",\n",
       " 'crossing',\n",
       " 'my',\n",
       " 'fingers',\n",
       " 'and',\n",
       " 'hoping',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'is',\n",
       " 'better.<br',\n",
       " '/><br',\n",
       " '/>the',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'i',\n",
       " 'can',\n",
       " 'recommend',\n",
       " 'is',\n",
       " 'not',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'the',\n",
       " 'madden',\n",
       " 'bundle.',\n",
       " 'the',\n",
       " 'game',\n",
       " 'comes',\n",
       " 'as',\n",
       " 'a',\n",
       " 'download.',\n",
       " 'so',\n",
       " 'if',\n",
       " 'you',\n",
       " 'hate',\n",
       " 'it,',\n",
       " \"there's\",\n",
       " 'no',\n",
       " 'trading',\n",
       " 'it',\n",
       " 'in',\n",
       " 'at',\n",
       " 'gamestop.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordTokenizeRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 7: To move ahead first we will clean our data, here we’re gonna remove stopwords, punctuations and empty spaces.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWordsFunct(x):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filteredSentence = [w for w in x if not w in stop_words]\n",
    "    return filteredSentence\n",
    "stopwordRDD = wordTokenizeRDD.map(removeStopWordsFunct)\n",
    "\n",
    "def removePunctuationsFunct(x):\n",
    "    list_punct=list(string.punctuation)\n",
    "    filtered = [''.join(c for c in s if c not in list_punct) for s in x] \n",
    "    filtered_space = [s for s in filtered if s] #remove empty space \n",
    "    return filtered\n",
    "rmvPunctRDD = stopwordRDD.map(removePunctuationsFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keep',\n",
       " 'buying',\n",
       " 'madden',\n",
       " 'every',\n",
       " 'year',\n",
       " 'hoping',\n",
       " 'get',\n",
       " 'back',\n",
       " 'football.',\n",
       " 'years',\n",
       " 'version',\n",
       " 'little',\n",
       " 'better',\n",
       " 'last',\n",
       " 'years',\n",
       " '--',\n",
       " \"that's\",\n",
       " 'saying',\n",
       " 'much.the',\n",
       " 'game',\n",
       " 'looks',\n",
       " 'great.',\n",
       " 'thing',\n",
       " 'wrong',\n",
       " 'animation,',\n",
       " 'way',\n",
       " 'players',\n",
       " 'always',\n",
       " 'tripping',\n",
       " 'other.<br',\n",
       " '/><br',\n",
       " '/>the',\n",
       " 'gameplay',\n",
       " 'still',\n",
       " 'slowed',\n",
       " 'bloated',\n",
       " 'pre-play',\n",
       " 'controls.',\n",
       " 'used',\n",
       " 'take',\n",
       " 'two',\n",
       " 'buttons',\n",
       " 'giant',\n",
       " 'pita',\n",
       " 'get',\n",
       " 'done',\n",
       " 'opponent',\n",
       " 'snaps',\n",
       " 'ball',\n",
       " 'play',\n",
       " 'clock',\n",
       " 'runs',\n",
       " 'out.<br',\n",
       " '/><br',\n",
       " '/>the',\n",
       " 'turbo',\n",
       " 'button',\n",
       " 'back,',\n",
       " 'player',\n",
       " 'movement',\n",
       " 'still',\n",
       " 'slow',\n",
       " 'awkward.',\n",
       " 'liked',\n",
       " 'last',\n",
       " 'years',\n",
       " 'version,',\n",
       " \"i'm\",\n",
       " 'guessing',\n",
       " 'like',\n",
       " 'too.',\n",
       " 'chance',\n",
       " 'play',\n",
       " 'anything',\n",
       " 'training',\n",
       " 'online',\n",
       " 'games,',\n",
       " \"i'm\",\n",
       " 'crossing',\n",
       " 'fingers',\n",
       " 'hoping',\n",
       " 'rest',\n",
       " 'better.<br',\n",
       " '/><br',\n",
       " '/>the',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'recommend',\n",
       " 'buy',\n",
       " 'madden',\n",
       " 'bundle.',\n",
       " 'game',\n",
       " 'comes',\n",
       " 'download.',\n",
       " 'hate',\n",
       " 'it,',\n",
       " \"there's\",\n",
       " 'trading',\n",
       " 'gamestop.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwordRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 8: Lemmatization**\n",
    "Stemming and Lemmatization are the basic text processing methods for English text. The goal of both of them is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. I have skipped Stemming because it is not an efficient method as sometimes it produces words that are not even close to the actual word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizationFunct(x):\n",
    "    nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    finalLem = [lemmatizer.lemmatize(s) for s in x]\n",
    "    return finalLem\n",
    "lem_wordsRDD = rmvPunctRDD.map(lemmatizationFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keep',\n",
       " 'buying',\n",
       " 'madden',\n",
       " 'every',\n",
       " 'year',\n",
       " 'hoping',\n",
       " 'get',\n",
       " 'back',\n",
       " 'football',\n",
       " 'year',\n",
       " 'version',\n",
       " 'little',\n",
       " 'better',\n",
       " 'last',\n",
       " 'year',\n",
       " '',\n",
       " 'thats',\n",
       " 'saying',\n",
       " 'muchthe',\n",
       " 'game',\n",
       " 'look',\n",
       " 'great',\n",
       " 'thing',\n",
       " 'wrong',\n",
       " 'animation',\n",
       " 'way',\n",
       " 'player',\n",
       " 'always',\n",
       " 'tripping',\n",
       " 'otherbr',\n",
       " 'br',\n",
       " 'the',\n",
       " 'gameplay',\n",
       " 'still',\n",
       " 'slowed',\n",
       " 'bloated',\n",
       " 'preplay',\n",
       " 'control',\n",
       " 'used',\n",
       " 'take',\n",
       " 'two',\n",
       " 'button',\n",
       " 'giant',\n",
       " 'pita',\n",
       " 'get',\n",
       " 'done',\n",
       " 'opponent',\n",
       " 'snap',\n",
       " 'ball',\n",
       " 'play',\n",
       " 'clock',\n",
       " 'run',\n",
       " 'outbr',\n",
       " 'br',\n",
       " 'the',\n",
       " 'turbo',\n",
       " 'button',\n",
       " 'back',\n",
       " 'player',\n",
       " 'movement',\n",
       " 'still',\n",
       " 'slow',\n",
       " 'awkward',\n",
       " 'liked',\n",
       " 'last',\n",
       " 'year',\n",
       " 'version',\n",
       " 'im',\n",
       " 'guessing',\n",
       " 'like',\n",
       " 'too',\n",
       " 'chance',\n",
       " 'play',\n",
       " 'anything',\n",
       " 'training',\n",
       " 'online',\n",
       " 'game',\n",
       " 'im',\n",
       " 'crossing',\n",
       " 'finger',\n",
       " 'hoping',\n",
       " 'rest',\n",
       " 'betterbr',\n",
       " 'br',\n",
       " 'the',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'recommend',\n",
       " 'buy',\n",
       " 'madden',\n",
       " 'bundle',\n",
       " 'game',\n",
       " 'come',\n",
       " 'download',\n",
       " 'hate',\n",
       " 'it',\n",
       " 'there',\n",
       " 'trading',\n",
       " 'gamestop']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_wordsRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 9: Our next task is a little tricky, we have to extract key phrases(also called Noun phrases). So first we need to join “lem_wordsRDD” tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinTokensFunct(x):\n",
    "    joinedTokens_list = []\n",
    "    x = \" \".join(x)\n",
    "    return x\n",
    "joinedTokens = lem_wordsRDD.map(joinTokensFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'keep buying madden every year hoping get back football year version little better last year  thats saying muchthe game look great thing wrong animation way player always tripping otherbr br the gameplay still slowed bloated preplay control used take two button giant pita get done opponent snap ball play clock run outbr br the turbo button back player movement still slow awkward liked last year version im guessing like too chance play anything training online game im crossing finger hoping rest betterbr br the one thing recommend buy madden bundle game come download hate it there trading gamestop'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedTokens.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 10: Extract key phrases, In the below code I’m doing chunking, chinking and POS tagging using Regular expression and extracting all the Noun phrases.**\n",
    "\n",
    "* **Chunking:** Also referred to as shallow parsing, is a task that follows Part-Of-Speech Tagging and that adds more structure to the sentence. The result is a grouping of the words in “chunks”.\n",
    "* **Chinking:** Chinking is a lot like chunking, it is basically a way for you to remove a chunk from a chunk. The chunk that you remove from your chunk is your chink.\n",
    "* **POS tagging:** A Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads the text in some language and assigns parts of speech to each word (and other tokens), such as noun, verb, adjective, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPhraseFunct(x):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    def leaves(tree):\n",
    "        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "            yield subtree.leaves()\n",
    "    \n",
    "    def get_terms(tree):\n",
    "        for leaf in leaves(tree):\n",
    "            term = [w for w,t in leaf if not w in stop_words]\n",
    "            yield term\n",
    "    sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\n",
    "    grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "    \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tokens = nltk.regexp_tokenize(x,sentence_re)\n",
    "    postoks = nltk.tag.pos_tag(tokens) #Part of speech tagging \n",
    "    tree = chunker.parse(postoks) #chunking\n",
    "    terms = get_terms(tree)\n",
    "    temp_phrases = []\n",
    "    for term in terms:\n",
    "        if len(term):\n",
    "            temp_phrases.append(' '.join(term))\n",
    "    \n",
    "    finalPhrase = [w for w in temp_phrases if w] #remove empty lists\n",
    "    return finalPhrase\n",
    "extractphraseRDD = joinedTokens.map(extractPhraseFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buying madden',\n",
       " 'year',\n",
       " 'football year version',\n",
       " 'last year thats',\n",
       " 'muchthe game look great thing wrong animation way player',\n",
       " 'gameplay',\n",
       " 'bloated preplay control',\n",
       " 'button giant pita',\n",
       " 'opponent snap ball play clock',\n",
       " 'turbo button',\n",
       " 'player movement',\n",
       " 'last year version',\n",
       " 'chance play anything',\n",
       " 'online game im',\n",
       " 'finger',\n",
       " 'rest betterbr',\n",
       " 'thing recommend',\n",
       " 'madden bundle game',\n",
       " 'download hate',\n",
       " 'gamestop']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractphraseRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 11: From the above step we roughly got all the key phrases the customers are talking about. Now categorize these key phrases into positive, Negative or Neutral.**\n",
    "There are so many ways to get the sentiments, I’m using NLTK VADER you guys can choose any other method as per your interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentWordsFunct(x):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer() \n",
    "    senti_list_temp = []\n",
    "    for i in x:\n",
    "        y = ''.join(i) \n",
    "        vs = analyzer.polarity_scores(y)\n",
    "        senti_list_temp.append((y, vs))\n",
    "        senti_list_temp = [w for w in senti_list_temp if w]\n",
    "    sentiment_list  = []\n",
    "    for j in senti_list_temp:\n",
    "        first = j[0]\n",
    "        second = j[1]\n",
    "    \n",
    "        for (k,v) in second.items():\n",
    "            if k == 'compound':\n",
    "                if v < 0.0:\n",
    "                    sentiment_list.append((first, \"Negative\"))\n",
    "                elif v == 0.0:\n",
    "                    sentiment_list.append((first, \"Neutral\"))\n",
    "                else:\n",
    "                    sentiment_list.append((first, \"Positive\"))\n",
    "     \n",
    "    return sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentRDD = extractphraseRDD.map(sentimentWordsFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buying madden',\n",
       " 'year',\n",
       " 'football year version',\n",
       " 'last year thats',\n",
       " 'muchthe game look great thing wrong animation way player',\n",
       " 'gameplay',\n",
       " 'bloated preplay control',\n",
       " 'button giant pita',\n",
       " 'opponent snap ball play clock',\n",
       " 'turbo button',\n",
       " 'player movement',\n",
       " 'last year version',\n",
       " 'chance play anything',\n",
       " 'online game im',\n",
       " 'finger',\n",
       " 'rest betterbr',\n",
       " 'thing recommend',\n",
       " 'madden bundle game',\n",
       " 'download hate',\n",
       " 'gamestop']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractphraseRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 12: Now let's extract the top 20 keywords from the extracted key phrases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 49, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 1861, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-13-70b5e1a2bce3>\", line 3, in <lambda>\nAttributeError: 'NoneType' object has no attribute 'lower'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 1861, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-13-70b5e1a2bce3>\", line 3, in <lambda>\nAttributeError: 'NoneType' object has no attribute 'lower'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-588c440bd6e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfreqDistRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractphraseRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msortBy\u001b[0;34m(self, keyfunc, ascending, numPartitions)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \"\"\"\n\u001b[0;32m--> 699\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mglom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msortByKey\u001b[0;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;31m# the key-space into bins such that the bins have roughly the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# number of (key, value) pairs falling into them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m         \u001b[0mrddSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrddSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# empty RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \"\"\"\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 49, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 1861, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-13-70b5e1a2bce3>\", line 3, in <lambda>\nAttributeError: 'NoneType' object has no attribute 'lower'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 1861, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    for k, v in iterator:\n  File \"/home/erikapat/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-13-70b5e1a2bce3>\", line 3, in <lambda>\nAttributeError: 'NoneType' object has no attribute 'lower'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "freqDistRDD = extractphraseRDD.flatMap(lambda x : nltk.FreqDist(x).most_common()).map(lambda x: x).reduceByKey(lambda x,y : x+y).sortBy(lambda x: x[1], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 13: Visualizing the output. I’m using the function pandD.plot.barh() to visualize the output. Click this to see all available graphs that can be used instead of a horizontal bar graph(barh).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'freqDistRDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-c5da42868790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_fDist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreqDistRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#converting RDD to spark dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_fDist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"myTable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT _1 AS Keywords, _2 as Frequency from myTable limit 20\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#renaming columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpandD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#converting spark dataframes to pandas dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpandD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Keywords'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Frequency'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'freqDistRDD' is not defined"
     ]
    }
   ],
   "source": [
    "df_fDist = freqDistRDD.toDF() #converting RDD to spark dataframe\n",
    "df_fDist.createOrReplaceTempView(\"myTable\") \n",
    "df2 = spark.sql(\"SELECT _1 AS Keywords, _2 as Frequency from myTable limit 20\") #renaming columns \n",
    "pandD = df2.toPandas() #converting spark dataframes to pandas dataframes\n",
    "pandD.plot.barh(x='Keywords', y='Frequency', rot=1, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
