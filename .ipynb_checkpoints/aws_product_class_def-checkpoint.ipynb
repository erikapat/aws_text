{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PRODUCT CATEGORY CLASSIFICATION**\n",
    "\n",
    "### Here, we want to develop an automatic and scalable first prototipe that helps to correctly categorize a new product in the available categories when it arrives.\n",
    "\n",
    "This first prototipe will help us to identify in what elements we have to go deeper in order to get the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28\n",
    "https://www.dataquest.io/blog/natural-language-processing-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary of requirements**\n",
    "\n",
    "1. Train a model that predicts the product category for Software, Digital Software, and\n",
    "Digital Video Games products using the Amazon Customer Reviews dataset.\n",
    "2. Evaluate and validate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I checked warnings, but for the final report I prefer ignore those \n",
    "#that really does not affect the results (warnings of libraries, etc)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "platform.architecture()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further improvement: develop an enviroment\n",
    "# Basic Modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "#from sagemaker.predictor import json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOdels\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules for tokens\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my own functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.py_functions import *\n",
    "from utils.cleaning_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DATA**\n",
    "For our problem we use will three datasets from public datasets of Amazon. This datasets contains Customer Reviews fro three category of products.\n",
    "\n",
    "* Software\n",
    "* Digital Software\n",
    "* Digital video games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!mkdir /tmp/recsys/\n",
    "!aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz /tmp/recsys/\n",
    "!aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Software_v1_00.tsv.gz /tmp/recsys/\n",
    "!aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz /tmp/recsys/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_video_games = pd.read_csv('data/amazon_reviews_us_Digital_Video_Games_v1_00.tsv', delimiter = '\\t', error_bad_lines = False)\n",
    "print('Number of rows: ', df_video_games.shape)\n",
    "df_software = pd.read_csv('data/amazon_reviews_us_Software_v1_00.tsv', delimiter = '\\t', error_bad_lines = False)\n",
    "print('Number of rows: ', df_software.shape)\n",
    "df_digital_software = pd.read_csv('data/amazon_reviews_us_Digital_Software_v1_00.tsv', delimiter = '\\t', error_bad_lines = False)\n",
    "print('Number of rows: ', df_digital_software.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_video_games.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset columns**\n",
    "\n",
    "* marketplace: 2-letter country\n",
    "* customer_id: random identifier for a customer\n",
    "* review_id: unique id for the review\n",
    "* product_id: ASIN number. Unique id for product\n",
    "* product_parent: the parent of that ASIN. Multiple ASINs (color or format variations of the same product) can roll up into a single parent parent.\n",
    "* product_title: title/description of the product\n",
    "* product_category: brad product category\n",
    "* star_rating: 1 to 5 stars (review rating)\n",
    "* helpful_votes: Number of helpful votes for the review\n",
    "* total_votes: number of total votes the review received\n",
    "* vine: Was the review written as part of the VIne program?\n",
    "* verified_purchase: Was the review from a verified purchase?\n",
    "* review_headline: the title of the review itself\n",
    "* review_body: text of the review\n",
    "* review_date: the date of the review was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_video_games.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DROP SOME COLUMNS**\n",
    "\n",
    "### When a new product arrives the only available information would be product title. A new product will not have rating, votes or reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_digital_software = df_digital_software.drop(columns = ['marketplace', 'customer_id', 'product_parent', 'vine', 'review_date'])\n",
    "df_software         = df_software.drop(columns = ['marketplace', 'customer_id', 'product_parent',  'vine', 'review_date'])\n",
    "df_video_games      = df_video_games.drop(columns = ['marketplace', 'customer_id', 'product_parent', 'vine', 'review_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_digital_software, df_video_games, df_software], axis=0)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CHECK IF WE HAVE MISSING VALUES**\n",
    "\n",
    "We have missing data in review_headline, review_body. We have to eliminate it or fix this in order to keep going with the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the na's are not at the same rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review_headline.fillna(df.review_body, inplace=True)\n",
    "df.review_body.fillna(df.review_headline, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check for white strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blanks = []  # start with an empty list\n",
    "\n",
    "for i,lb,rv in df[['product_category','review_body']].itertuples():  # iterate over the DataFrame , 'product_title', 'verified_purchase', 'review_headline'\n",
    "    if type(rv)==str:            # avoid NaN values\n",
    "        if rv.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list\n",
    "        \n",
    "print(len(blanks), 'blanks: ', blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blanks = []  # start with an empty list\n",
    "\n",
    "for i,lb,rv in df[['product_title', 'review_headline']].itertuples():  # iterate over the DataFrame , \n",
    "    if type(rv)==str:            # avoid NaN values\n",
    "        if rv.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list\n",
    "        \n",
    "print(len(blanks), 'blanks: ', blanks)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check duplicates**\n",
    "\n",
    "We can se that we have several duplicated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.drop(columns = ['review_id']).duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['review_headline'] == 'Great on the Surface Pro 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, we eliminate them\n",
    "print(df.shape)\n",
    "df = df.drop_duplicates(subset = ['product_id', 'product_title', 'product_category', 'star_rating', 'helpful_votes', 'total_votes',  'verified_purchase', \n",
    "                                     'review_headline', 'review_body'], keep = 'first')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset = ['product_id', 'product_title', 'product_category', 'review_headline', 'review_body'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IS BALANCED?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*df.groupby(\"product_category\")['product_id'].count()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicti = {\"Digital_Software\": 0, \"Digital_Video_Games\": 1, \"Software\": 2}\n",
    "df['product_category_label'] = df['product_category']\n",
    "df = df.replace({\"product_category\": dicti})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FILTER VALUES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['product_id', 'helpful_votes'], ascending=True)\n",
    "#df = df.drop_duplicates(subset = ['product_title'], keep = 'first') # only products\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PRODUCT TITLE**\n",
    "\n",
    "### **INCLUDE NUMERICAL INFO ABOUT THE NAME OF THE PRODUCT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include other features\n",
    "import re\n",
    "# Our list of functions to apply.\n",
    "transform_functions = [\n",
    "    lambda x: len(x),\n",
    "    lambda x: x.count(\" \"),\n",
    "    lambda x: x.count(\".\"),\n",
    "    lambda x: x.count(\"!\"),\n",
    "    lambda x: x.count(\"?\"),\n",
    "    lambda x: len(x) / (x.count(\" \") + 1),\n",
    "    lambda x: x.count(\" \") / (x.count(\".\") + 1),\n",
    "    lambda x: len(re.findall(\"CD|DVD\", x)), # CD \n",
    "    lambda x: len(re.findall(r\"\\d+st|\\d+th|\\d+sd\", x)), # th--> 4th, 5th or 1st or 2sd\n",
    "    lambda x: len(re.findall(\"[A-Z]\", x)), # number of uppercase letters\n",
    "    lambda x: len(re.findall(\"[0-9]\", x)), #numbers\n",
    "    lambda x: len(re.findall(\"\\d{4}\", x)),\n",
    "    lambda x: len(re.findall(\"\\d$\", x)), #end with number\n",
    "    lambda x: len(re.findall(\"^\\d\", x)), #start with number\n",
    "    lambda x: len(re.findall(\"[\\w]+-[\\w]+\",x)), #words separated with -\n",
    "    lambda x: len(re.findall(\"OLD VERSION|Old Version|old version\",x)), #old version\n",
    "]\n",
    "\n",
    "transform_functions_len = [\n",
    "    lambda x: len(x)\n",
    "]\n",
    "\n",
    "# Apply each function and put the results into a list.\n",
    "df_num = df[['review_id', 'product_id', 'product_title', 'star_rating', 'helpful_votes', 'total_votes', 'verified_purchase']]\n",
    "df_num_2 = df[['review_id', 'product_id', 'product_title']]\n",
    "df = df.drop(columns = ['star_rating', 'helpful_votes', 'total_votes', 'verified_purchase'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func in transform_functions:\n",
    "     df_num_2 = pd.concat([df_num_2, df['product_title'].apply(func)], axis=1)\n",
    "df_num_2 = pd.concat([df_num_2, df['review_body'].apply(transform_functions_len)], axis=1)\n",
    "df_num_2 = pd.concat([df_num_2, df['review_headline'].apply(transform_functions_len)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_2.columns = ['review_id', 'product_id', 'product_title', 'title_len', 'title_words', 'title_points',\n",
    "                  'title_exc', 'title_int', 'ratio_spaces_point', \n",
    "                  'ratio_len_points', 'title_cd','title_th', 'title_upper_letters', 'title_numbers', 'title_years', 'end_number', 'starts_number', 'word_sep', \n",
    "                  'title_old_version', 'len_review_body', 'len_review_headline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Combine tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "dfs = [df, df_num, df_num_2]\n",
    "df = reduce(lambda left,right: pd.merge(left,right,on=['review_id', 'product_id', 'product_title'], how = 'inner'), dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Some descriptive values**\n",
    "\n",
    "* How do ratings vary by product category?\n",
    "\n",
    "It appears that Digital Software is the most disappointing category for reviewers, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['product_category'])['star_rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['product_category'])['helpful_votes'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['product_category'])['helpful_votes'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can observe, more than 75% of reviews have not been found helpful by people, and among those who were found helpful, the review text has an average length of more than 150 words. While most of the reviews were given 4 or 5 stars, there is no clear pattern between the rating and the length of the reviews. \n",
    "\n",
    "So, longer reviews are more helpful, but it does not mean people liked the product more? Well, we need a better way to untangle this mess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['star_rating'])['title_len'].mean().reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"star_rating\", y=\"title_len\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"star_rating\", y=\"len_review_headline\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"star_rating\", y=\"len_review_body\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['star_rating'])['helpful_votes'].mean().reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"star_rating\", y=\"helpful_votes\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text data processing**\n",
    "\n",
    "We cleaned the review text to remove all ‘noise’ (in this case, that translates to punctuation, junk values and upper case letters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean characters\n",
    "df = standardize_text(df, \"product_title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Eliminate stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords with nltk.\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "df['product_title'] = df['product_title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#df['review_headline'] = df['review_headline'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#df['review_body'] = df['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#more cleaning\n",
    "df['product_title'] = df['product_title'].map(lambda x : clean_text(x))\n",
    "#df['review_headline'] = df['review_headline'].map(lambda x : clean_text(x))\n",
    "#df['review_body'] = df['review_body'].map(lambda x : clean_text(x))\n",
    "df['product_title'] = df['product_title'].map(lambda x : removeAscendingChar(x)) \n",
    "#df['review_headline'] = df['review_headline'].map(lambda x : removeAscendingChar(x)) \n",
    "#df['review_body'] = df['review_body'].map(lambda x : removeAscendingChar(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lemmatization**\n",
    "\n",
    "Reduce the inflectional forms of each word into a common base or root. For example, it’s very likely we will want to see results containing the form “skirt” if we have typed “skirts” in a search bar.\n",
    "\n",
    "![alt text](fig/lemma.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "df['product_title'] = df['product_title'].map(lambda x : lemitizeWords(x))\n",
    "df['review_headline'] = df['review_headline'].map(lambda x : lemitizeWords(x))\n",
    "df['review_body'] = df['review_body'].map(lambda x : lemitizeWords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminate single letters\n",
    "df['product_title'] = df['product_title'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "df['review_headline'] = df['review_headline'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "df['review_body'] = df['review_body'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Spelling correction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "#df['product_title'] = df['product_title'].apply(lambda tweet: TextBlob(tweet).correct())\n",
    "#df['review_headline'] = df['review_headline'].apply(lambda tweet: TextBlob(tweet).correct())\n",
    "#df['review_body'] = df['review_body'].apply(lambda tweet: TextBlob(tweet).correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The objective/subjective part of the sentence: Textblob package**\n",
    "\n",
    "\n",
    "* To install:\n",
    "\n",
    "pip install -U textblob\n",
    "\n",
    "python -m textblob.download_corpora\n",
    "\n",
    "With the Textblob package we can eliminate the objective/subjective part of a setence. Why do this? Think about it. Would you rather read ‘This iPhone has 64 gb of storage’ or ‘This iPhone has high storage for low price’? Depending of the utility of the project you would need the objective or Subjectivity part. Textblob does a good job of telling you how subjective a sentence is, using which you can eliminate those that fall below a certain threshold \n",
    "\n",
    "In our case we want to eliminate subjectivity an keep those sentences that give us more objective information about the product. We picked a threshold below 0.3. \n",
    "\n",
    "More info in: https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "#df['review_body'].apply(lambda tweet: TextBlob(tweet).sentiment) # (polarity, subjectivity) \n",
    "#xx = df['review_body'].apply(lambda tweet: TextBlob(tweet).noun_phrases) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_body'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BASELINE MODEL** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will develop a simple model, that we are going to use as Baseline.\n",
    "\n",
    "For our baseline model we will do:\n",
    "* Tokenizing sentences to a list of separate words\n",
    "* Creating a train test split\n",
    "* Inspecting our data a little more to validate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df[\"token_product_title\"] = df[\"product_title\"].apply(tokenizer.tokenize)\n",
    "df[\"token_review_body\"] = df[\"review_body\"].apply(tokenizer.tokenize)\n",
    "df[\"token_review_headline\"] = df[\"review_headline\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Inspecting tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths_1 = descriptive_tokens(df, name = \"token_product_title\")\n",
    "sentence_lengths_2 = descriptive_tokens(df, name = \"token_review_body\")\n",
    "sentence_lengths_3 = descriptive_tokens(df, name = \"token_review_headline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig= plt.figure(figsize=(10, 5)) \n",
    "plt.subplot(1, 3, 1) \n",
    "plt.xlabel('Sentence length')\n",
    "plt.ylabel('Number of sentences product title')\n",
    "plt.hist(sentence_lengths_1)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.xlabel('Sentence length')\n",
    "plt.ylabel('Number of sentences review body')\n",
    "plt.hist(sentence_lengths_2)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.xlabel('Sentence length')\n",
    "plt.ylabel('Number of sentences review headline')\n",
    "plt.hist(sentence_lengths_3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **concatenate columns**\n",
    "\n",
    "#We will use a concatination of columns, to see if we include review information or not\n",
    "\n",
    "#\n",
    "#df['full_text'] = df[\"product_title\"] + \" \" + df[\"review_body\"]  + \" \" + df[\"review_headline\"]\n",
    "df['product_title'] = df[\"product_title\"] #+ \" \" + df[\"review_body\"] #  + \" \" + df[\"review_headline\"] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Enter embeddings**\n",
    "\n",
    "Machine Learning on images can use raw pixels as inputs. A way to represent text is to encode each character individually, this seems quite inadequate to represent and understand language. Our goal is to first create a useful embedding for each sentence in our dataset, and then use these embeddings to accurately predict the relevant category.\n",
    "\n",
    "The most simplest approach is to use a **bag of words model**, and apply a machine learning algorimth (like, logistic, naive bayes, between others). A bag of words just associates an index to each word in our vocabulary, and embeds each sentence as a list of 0s, with a 1 at each index corresponding to a word present in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag of Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **combine numerica and nonnumeric data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df['product_title'] = df['review_headline']\n",
    "cols = ['product_title', 'title_len', 'title_words', 'title_points', 'title_exc', 'title_int',\n",
    "       'ratio_spaces_point', 'ratio_len_points', 'title_cd', 'title_th',\n",
    "       'title_upper_letters', 'title_numbers', 'title_years', 'end_number',\n",
    "       'starts_number', 'word_sep', 'title_old_version', 'len_review_body',\n",
    "       'len_review_headline', 'token_product_title', 'token_review_body',\n",
    "       'token_review_headline']\n",
    "\n",
    "df_corpus = df[cols]\n",
    "df_labels = df[\"product_category\"] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_corpus, df_labels, test_size=0.2, \n",
    "                                                                                random_state=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts().plot('barh') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def cv(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "\n",
    "    return emb, count_vectorizer\n",
    "\n",
    "X_train_counts, count_vectorizer = cv(X_train['product_title'])\n",
    "X_test_counts = count_vectorizer.transform(X_test['product_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_counts.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CHI-SQUARED**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve constructed a matrix, that have a lot of unique words/columns. This data configuratio will take a very long time to make predictions. We want to speed it up, so we’ll need to cut down the column count somehow. One way to do this is to pick a subset of the columns that are the most informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "# Find the 1000 most informative columns\n",
    "selector = SelectKBest(chi2, k=1000) #further work:check other alternatives\n",
    "selector.fit(X_train_counts, y_train)\n",
    "top_words = selector.get_support().nonzero()\n",
    "\n",
    "# Pick only the most informative columns in the data.\n",
    "chi_matrix = X_train_counts[:,top_words[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chi_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts = chi_matrix.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOP WORDS FOR TEST\n",
    "X_test_counts = X_test_counts[:,top_words[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_counts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NUMERICAL VARIABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['title_len', 'title_words', 'title_points', 'title_exc', 'title_int',\n",
    "       'ratio_spaces_point', 'ratio_len_points', 'title_cd', 'title_th',\n",
    "       'title_upper_letters', 'title_numbers', 'title_years', 'end_number',\n",
    "       'starts_number', 'word_sep', 'title_old_version', 'len_review_body',\n",
    "       'len_review_headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train[cols]\n",
    "X_test_num = X_test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(X_train_counts.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_counts = np.hstack([X_train_counts.todense(),np.asarray(X_train_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_counts = np.hstack([X_test_counts.todense(), np.asarray(X_test_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualizing the embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we apply linear dimensionality reduction to see if we can find separations between the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))          \n",
    "plot_LSA(X_train_counts, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These embeddings don't look very cleanly separated. Let's see if we can still fit a useful model on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fitting a classifier**\n",
    "Starting with a logistic regression is a good idea. It is simple, often gets the job done, and is easy to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance classes \n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "classes = np.unique(y_train)\n",
    "class_weights = dict(zip(classes, class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=30.0, class_weight=class_weights, #class_weight = 'balanced', \n",
    "                         solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=1, random_state=40) #, n_jobs=-1 (default) all, -2 all cpus but one are used\n",
    "clf.fit(X_train_counts, y_train) #, class_weight=class_weights to balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_counts = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_predicted_counts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Let's start by looking at some metrics to see if our classifier performed well at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inspection**\n",
    "A metric is one thing, but in order to make an actionnable decision, we need to actually inspect the kind of mistakes our classifier is making. Let's start by looking at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predicted_counts)\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "plot = plot_confusion_matrix(cm, classes=['Digital_Software', 'Digital_Video_Games', 'Software'], normalize=True, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANCE OF WORDS\n",
    "importance = get_most_important_features(count_vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance[1]['tops'] #tops for video games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance[0]['tops'] #digital software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance[2]['tops'] #software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video games (1)\n",
    "top_scores_video = [a[0] for a in importance[1]['tops']]\n",
    "top_words_video = [a[1] for a in importance[1]['tops']]\n",
    "#digital software (0)\n",
    "top_scores_digital = [a[0] for a in importance[0]['tops']]\n",
    "top_words_digital = [a[1] for a in importance[0]['tops']]\n",
    "##software\n",
    "top_scores_software = [a[0] for a in importance[2]['tops']]\n",
    "top_words_software = [a[1] for a in importance[2]['tops']]\n",
    "\n",
    "print(\"Most important words\")\n",
    "plot_important_words(top_scores_video, top_words_video, name = 'Digital video Games')\n",
    "plot_important_words(top_scores_digital, top_words_digital, name = 'Digital Software')\n",
    "plot_important_words(top_scores_software, top_words_software, name = 'Software')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TFIDF Bag of Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a slightly more subtle approach. Now, we will use a TF-IDF (Term Frequency, Inverse Document Frequency) which means weighing words by how frequent they are in our dataset, discounting words that are too frequent (as of, the and others), because they just add noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer\n",
    "\n",
    "X_train_tfidf, tfidf_vectorizer = tfidf(X_train['product_title'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['product_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))          \n",
    "plot_LSA(X_train_tfidf, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tfidf = LogisticRegression(C=30.0, class_weight= class_weights,# 'balanced', \n",
    "                               solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=1, random_state=40)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_predicted_tfidf = clf_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf = get_metrics(y_test, y_predicted_tfidf)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(y_test, y_predicted_tfidf)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm2, classes=['Digital_Software', 'Digital_Video_Games', 'Software'], normalize=True, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(\"TFIDF confusion matrix\")\n",
    "print(cm2)\n",
    "print(\"BoW confusion matrix\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at important coefficients of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_tfidf = get_most_important_features(tfidf_vectorizer, clf_tfidf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video games (1)\n",
    "top_scores_video = [a[0] for a in importance_tfidf[1]['tops']]\n",
    "top_words_video = [a[1] for a in importance_tfidf[1]['tops']]\n",
    "#digital software (0)\n",
    "top_scores_digital = [a[0] for a in importance_tfidf[0]['tops']]\n",
    "top_words_digital = [a[1] for a in importance_tfidf[0]['tops']]\n",
    "##software\n",
    "top_scores_software = [a[0] for a in importance_tfidf[2]['tops']]\n",
    "top_words_software = [a[1] for a in importance_tfidf[2]['tops']]\n",
    "\n",
    "print(\"Most important words\")\n",
    "plot_important_words(top_scores_video, top_words_video, name = 'Digital video Games') \n",
    "plot_important_words(top_scores_digital, top_words_digital, name = 'Digital Software')\n",
    "plot_important_words(top_scores_software, top_words_software, name = 'Software')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words the model picked up look much more relevant! Although our metrics on our held out validation set haven't increased much, we have much more confidence in the terms our model is using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **word2vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing semantic meaning\n",
    "Our first models have managed to pick up on high signal words. However, it is unlikely that we will have a training set containing all relevant words. To solve this problem, we need to capture the semantic meaning of words. \n",
    "\n",
    "### Enter word2vec\n",
    "Word2vec is a model that was pre-trained on a very large set of sentences, and provides embeddings that map words that are similar close to each other. A quick way to get a sentence embedding for our classifier, is to average word2vec scores of all words in our sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install --upgrade pip\n",
    "\n",
    "#!wget https://github.com/mmihaltz/word2vec-GoogleNews-vectors/blob/master/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The load_word2vec_format() method also has an optional limit argument which will only load the supplied number of vectors – so you could use limit=500000 to cut the memory requirements by about 5/6ths. (And, since the GoogleNews and other vector sets are usually ordered from most- to least-frequent words, you'll get the 500K most-frequent words. Lower-frequency words generally have much less value and even not-as-good vectors, so it may not hurt much to ignore them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit = 100000)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data to introduce in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_labels = df[\"product_category\"].tolist()\n",
    "df_corpus2 = df[[\"product_category\", 'product_title', 'token_product_title']] # introduce more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_word2vec_embeddings(word2vec, df_corpus2, name = 'token_product_title')\n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, list_labels, \n",
    "                                                                                        test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))          \n",
    "plot_LSA(embeddings, list_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look a little bit more separated, let's see how our logistic regression does on them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_w2v = LogisticRegression(C=30.0, class_weight = class_weights, #class_weight='balanced', \n",
    "                             solver = 'newton-cg', \n",
    "                         multi_class = 'multinomial', random_state = 40)\n",
    "clf_w2v.fit(X_train_word2vec, y_train_word2vec)\n",
    "y_predicted_word2vec = clf_w2v.predict(X_test_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, \n",
    "                                                                       recall_word2vec, f1_word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_w2v = confusion_matrix(y_test_word2vec, y_predicted_word2vec)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot = plot_confusion_matrix(cm, classes=['Digital_Software', 'Digital_Video_Games', 'Software'], normalize=True, title='Confusion matrix')\n",
    "plt.show()\n",
    "print(\"Word2Vec confusion matrix\")\n",
    "print(cm_w2v)\n",
    "print(\"TFIDF confusion matrix\")\n",
    "print(cm2)\n",
    "print(\"BoW confusion matrix\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **More values for the logistic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicti_n = {0:\"Digital_Software\", 1:\"Digital_Video_Games\", 2:\"Software\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf_w2v.score(X, y)\n",
    "arr = clf_w2v.predict_proba(X_test_word2vec)\n",
    "df_predict= pd.DataFrame(data=arr)\n",
    "df_predict.columns = [\"Digital_Software\", \"Digital_Video_Games\", \"Software\"]\n",
    "arr = clf_w2v.predict(X_test_word2vec)\n",
    "df_predict['predicted_label'] = pd.DataFrame(data=arr.flatten())\n",
    "df_predict['prediction_title'] = df_predict['predicted_label'].copy()\n",
    "df_predict = df_predict.replace({\"prediction_title\": dicti_n})\n",
    "arr = np.array(y_test_word2vec)\n",
    "df_predict['real_label'] = pd.DataFrame(data=arr.flatten())\n",
    "df_predict[\"max_pred\"] = df_predict[['Digital_Software', 'Digital_Video_Games','Software']].max(axis=1)\n",
    "print(df_predict.shape)\n",
    "df_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict[df_predict['max_pred'] <.60].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_long = pd.melt(df_predict, id_vars=['predicted_label', 'prediction_title'])\n",
    "df_predict_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.kdeplot(df_predict_long.loc[(df_predict_long['variable']=='Software'), \n",
    "            'value'], color='r', shade=True, Label='Software') \n",
    "sns.kdeplot(df_predict_long.loc[(df_predict_long['variable']=='Digital_Software'), \n",
    "            'value'], color='b', shade=True, Label='Digital_Software')   \n",
    "sns.kdeplot(df_predict_long.loc[(df_predict_long['variable']=='Digital_Video_Games'), \n",
    "            'value'], color='b', shade=True, Label='Digital_Video_Games')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further inspection\n",
    "In order to provide some explainability, we can leverage a black box explainer such as LIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "df_corpus = df['product_title'] #df['review_headline']#df['full_text']\n",
    "#df_corpus = df['review_body', 'review_headline']\n",
    "df_labels = df[\"product_category\"] \n",
    "\n",
    "\n",
    "X_train_data, X_test_data, y_train_data, y_test_data = train_test_split(df_corpus, df_labels, test_size=0.2, \n",
    "                                                                                random_state=40)\n",
    "vector_store = word2vec\n",
    "def word2vec_pipeline(examples):\n",
    "    global vector_store\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_list = []\n",
    "    for example in examples:\n",
    "        example_tokens = tokenizer.tokenize(example)\n",
    "        vectorized_example = get_average_word2vec(example_tokens, vector_store, generate_missing=False, k=300)\n",
    "        tokenized_list.append(vectorized_example)\n",
    "    return clf_w2v.predict_proba(tokenized_list)\n",
    "\n",
    "c = make_pipeline(count_vectorizer, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_one_instance(instance, class_names):\n",
    "    '''\n",
    "    num_features: num of features to use to explain\n",
    "    top_labels: number of labels to use\n",
    "    '''\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "    exp = explainer.explain_instance(instance, word2vec_pipeline, num_features=15,  top_labels=2)\n",
    "    return exp\n",
    "\n",
    "def visualize_one_exp(features, labels, index, class_names):\n",
    "    exp = explain_one_instance(features[index], class_names = class_names)\n",
    "    print('Index: %d' % index)\n",
    "    print('True class: %s' % class_names[labels[index]])\n",
    "    exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Digital_Software', 'Digital_Video_Games', 'Software']\n",
    "visualize_one_exp(X_test_data, y_test_data, 54, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_one_exp(X_test_data, y_test_data, 160, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_one_exp(X_test_data, y_test_data, 31937, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_text = {\n",
    "    0: \"Digital_Software\", \n",
    "    1: \"Digital_Video_Games\", \n",
    "    2: \"Software\"\n",
    "}\n",
    "random.seed(40)\n",
    "#importance of words\n",
    "sorted_contributions = get_statistical_explanation(X_test_data.tolist(), 100, word2vec_pipeline, label_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Software\n",
    "top_words_Software = sorted_contributions['Software']['tops'][:10].index.tolist()\n",
    "top_scores_Software = sorted_contributions['Software']['tops'][:10].tolist()\n",
    "top_words_GAMES = sorted_contributions['Digital_Video_Games']['tops'][:10].index.tolist()\n",
    "top_scores_GAMES = sorted_contributions['Digital_Video_Games']['tops'][:10].tolist()\n",
    "top_words_Digital = sorted_contributions['Digital_Software']['tops'][:10].index.tolist()\n",
    "top_scores_Digital = sorted_contributions['Digital_Software']['tops'][:10].tolist()\n",
    "\n",
    "plot_important_words(top_scores_Software, top_words_Software, \"Software\")\n",
    "plot_important_words(top_scores_GAMES, top_words_GAMES, \"Digital Video Games\")\n",
    "plot_important_words(top_scores_Digital, top_words_Digital, 'Digital Software')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion:**\n",
    "\n",
    "We get better results with **TFIDF Bag of Words** option. Also, aggregate review info to the model does not help to the improvement of the results of the model that only consider 'product title'. Further analysis are required to get a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NEXT STEPS**\n",
    "\n",
    "### Based in the results there are several things to do to improve the results:\n",
    "1. Improve the text treatment, there are words that have misspelling or not sense. More time cleaning it is necessary. Also, it is necessary to spend more time cleaning reviews (eliminate adjectives, verbs, keep only noums that can help to identify the category).\n",
    "2. Use n_grams > 1. A word by itself may mean nothing, specially when we are treating with names of products.\n",
    "3. This first approach was mostly exploratory, for this reason we only use logistic regression, but there are models that may help to improve the results (based tree methodologies, deep leaning, bayesian methods, between others), aditionally would be interesting combine different methodolofies using emsemble methodologies like (max voting, averaging, bagging or boosting). Compare the results with ROC curves and information criteria as AIC and BIC.\n",
    "4. We can try to improve the results combining text with numerical variables. Use number of votes to give more weights to those reviews with more votes may help to find better keywords for the products. Include a label for relevant/irrelenvant information based in the available information.\n",
    "5. Use Spark to speed the results.\n",
    "6. Use methodologies as SMOTE to balance the data.\n",
    "7. Explore other alternative to Word2Vect like: fastText GloVe or ELMO. Consider other alternatives like pre-trained NLP models to get better results, as for example BERT, ULMFIT, between others.\n",
    "8. Use BlazingText’s implemetation of Word2Vec.\n",
    "9. Use H2OAutoML algorithm to if we can improve the results.\n",
    "10. INclude more categories to the analysis.\n",
    "11. Implement the ideas wrote at the beggining to fight misclassification issues.\n",
    "12. During the treatment we eliminate numbers, because we think this may aggregate noise to result. We may recheck this treatment and evaluate which may remain in the trining dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
